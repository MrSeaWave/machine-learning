# Gradient Descent

## Tuning your learning rates

- Popular & Simple Idea: Reduce the learning rate by
some factor every few epochs.

    - At the beginning, we are far from the destination, so we
    use larger learning rate
    - After several epochs, we are close to the destination, so
    we reduce the learning rate

- Learning rate cannot be one-size-fits-all
    - Giving different parameters different learning
    rates 
